{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VAE Variational autoencoders.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM0w1jDB0VHZRvWxJj+PvGB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/k589k589/DeepLearning/blob/main/VAE_Variational_autoencoders.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FhTKRvUOYJhS"
      },
      "source": [
        "GAN 和 VAE 是兩種不同的學習圖像表示潛在空間的策略，每種策略都有自己的特點。\n",
        "VAE 非常適合學習結構良好的潛在空間，其中特定方向編碼數據中有意義的變化軸。\n",
        "GAN 生成的圖像可能非常逼真，但它們來自的潛在空間可能沒有那麼多的結構和連續性。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjeFaOv6YSD_"
      },
      "source": [
        "# 從技術角度來說，VAE 的工作原理如下：\n",
        "\n",
        "編碼器模塊將輸入樣本input_img轉換為潛在表示空間中的兩個參數，z_mean和z_log_variance。\n",
        "您z從假定生成輸入圖像的潛在正態分佈中隨機採樣一個點，通過z = z_mean + exp(z_log_variance) * epsilon，其中epsilon是小值的隨機張量。\n",
        "解碼器模塊將潛在空間中的這一點映射回原始輸入圖像。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wbE6raV9UvlR"
      },
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "latent_dim = 2\n",
        "\n",
        "encoder_inputs = keras.Input(shape=(28, 28, 1))\n",
        "x = layers.Conv2D(32, 3, activation=\"relu\", strides=2, padding=\"same\")(encoder_inputs)\n",
        "x = layers.Conv2D(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
        "x = layers.Flatten()(x)\n",
        "x = layers.Dense(16, activation=\"relu\")(x)\n",
        "z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n",
        "z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n",
        "encoder = keras.Model(encoder_inputs, [z_mean, z_log_var], name=\"encoder\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0oI4YF2aWnt8",
        "outputId": "b6fdf6fd-c953-4895-b10f-56508d39b35e"
      },
      "source": [
        "encoder.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"encoder\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 28, 28, 1)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 14, 14, 32)   320         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 7, 7, 64)     18496       conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "flatten (Flatten)               (None, 3136)         0           conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 16)           50192       flatten[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "z_mean (Dense)                  (None, 2)            34          dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "z_log_var (Dense)               (None, 2)            34          dense[0][0]                      \n",
            "==================================================================================================\n",
            "Total params: 69,076\n",
            "Trainable params: 69,076\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MD7IKeRoXdW5"
      },
      "source": [
        "# Latent-space-sampling layer\n",
        "使用z_mean和的代碼，z_log_var假設已經產生了統計分佈的參數input_img，以生成潛在空間點z。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TdzOHpHPXc9K"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "class Sampler(layers.Layer):\n",
        "    def call(self, z_mean, z_log_var):\n",
        "        batch_size = tf.shape(z_mean)[0]\n",
        "        z_size = tf.shape(z_mean)[1]\n",
        "        epsilon = tf.random.normal(shape=(batch_size, z_size))\n",
        "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Rjd4Di6Y05L"
      },
      "source": [
        "下面的清單顯示了解碼器的實現。我們將向量重塑為z圖像的尺寸，然後使用幾個卷積層獲得與原始尺寸相同的最終圖像輸出input_img。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_p_KbI-YkCz"
      },
      "source": [
        "#VAE解碼器網絡，將潛在空間點映射到圖像\n",
        "latent_inputs = keras.Input(shape=(latent_dim,))\n",
        "x = layers.Dense(7 * 7 * 64, activation=\"relu\")(latent_inputs)\n",
        "x = layers.Reshape((7, 7, 64))(x)\n",
        "x = layers.Conv2DTranspose(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
        "x = layers.Conv2DTranspose(32, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
        "decoder_outputs = layers.Conv2D(1, 3, activation=\"sigmoid\", padding=\"same\")(x)\n",
        "decoder = keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjxzY5dSZDY8"
      },
      "source": [
        "# 自定義的 VAE 模型train_step()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ddyN2KMY-Ul"
      },
      "source": [
        "class VAE(keras.Model):\n",
        "    def __init__(self, encoder, decoder, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.sampler = Sampler()\n",
        "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
        "        self.reconstruction_loss_tracker = keras.metrics.Mean(\n",
        "            name=\"reconstruction_loss\")\n",
        "        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [self.total_loss_tracker,\n",
        "                self.reconstruction_loss_tracker,\n",
        "                self.kl_loss_tracker]\n",
        "\n",
        "    def train_step(self, data):\n",
        "        with tf.GradientTape() as tape:\n",
        "            z_mean, z_log_var = self.encoder(data)\n",
        "            z = self.sampler(z_mean, z_log_var)\n",
        "            reconstruction = decoder(z)\n",
        "            reconstruction_loss = tf.reduce_mean(\n",
        "                tf.reduce_sum(\n",
        "                    keras.losses.binary_crossentropy(data, reconstruction),\n",
        "                    axis=(1, 2)\n",
        "                )\n",
        "            )\n",
        "            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
        "            total_loss = reconstruction_loss + tf.reduce_mean(kl_loss)\n",
        "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
        "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
        "        self.total_loss_tracker.update_state(total_loss)\n",
        "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
        "        self.kl_loss_tracker.update_state(kl_loss)\n",
        "        return {\n",
        "            \"total_loss\": self.total_loss_tracker.result(),\n",
        "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
        "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
        "        }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YLq7GpGlZG08"
      },
      "source": [
        "import numpy as np\n",
        "#let's train it\n",
        "(x_train, _), (x_test, _) = keras.datasets.mnist.load_data()\n",
        "mnist_digits = np.concatenate([x_train, x_test], axis=0)\n",
        "mnist_digits = np.expand_dims(mnist_digits, -1).astype(\"float32\") / 255\n",
        "\n",
        "vae = VAE(encoder, decoder)\n",
        "vae.compile(optimizer=keras.optimizers.Adam(), run_eagerly=True)\n",
        "vae.fit(mnist_digits, epochs=30, batch_size=128)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Bva892nZWon"
      },
      "source": [
        "# 從 2D 潛在空間採樣點網格並將它們解碼為圖像"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cs_v4bEjZUJN"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "n = 30\n",
        "digit_size = 28\n",
        "figure = np.zeros((digit_size * n, digit_size * n))\n",
        "\n",
        "grid_x = np.linspace(-1, 1, n)\n",
        "grid_y = np.linspace(-1, 1, n)[::-1]\n",
        "\n",
        "for i, yi in enumerate(grid_y):\n",
        "    for j, xi in enumerate(grid_x):\n",
        "        z_sample = np.array([[xi, yi]])\n",
        "        x_decoded = vae.decoder.predict(z_sample)\n",
        "        digit = x_decoded[0].reshape(digit_size, digit_size)\n",
        "        figure[\n",
        "            i * digit_size : (i + 1) * digit_size,\n",
        "            j * digit_size : (j + 1) * digit_size,\n",
        "        ] = digit\n",
        "\n",
        "plt.figure(figsize=(15, 15))\n",
        "start_range = digit_size // 2\n",
        "end_range = n * digit_size + start_range\n",
        "pixel_range = np.arange(start_range, end_range, digit_size)\n",
        "sample_range_x = np.round(grid_x, 1)\n",
        "sample_range_y = np.round(grid_y, 1)\n",
        "plt.xticks(pixel_range, sample_range_x)\n",
        "plt.yticks(pixel_range, sample_range_y)\n",
        "plt.xlabel(\"z[0]\")\n",
        "plt.ylabel(\"z[1]\")\n",
        "plt.axis(\"off\")\n",
        "plt.imshow(figure, cmap=\"Greys_r\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jzfevKikZjrE"
      },
      "source": [
        "使用深度學習生成圖像是通過學習捕獲有關圖像數據集的統計信息的潛在空間來完成的。通過從潛在空間中採樣和解碼點，您可以生成前所未見的圖像。有兩個主要工具可以做到這一點：VAE 和 GAN。\n",
        "\n",
        "VAE 導致高度結構化、連續的潛在表示。出於這個原因，它們適用於在潛在空間中進行各種圖像編輯：換臉、將皺眉的臉變成笑臉等等。它們也可以很好地用於製作基於潛在空間的動畫，例如沿著潛在空間的橫截面動畫行走，顯示起始圖像以連續方式緩慢變形為不同圖像。\n",
        "\n",
        "GAN 能夠生成逼真的單幀圖像，但可能不會產生具有堅實結構和高度連續性的潛在空間。\n",
        "\n",
        "我見過的大多數成功的圖像實際應用都依賴於 VAE，但 GAN 在學術研究領域一直很受歡迎。您將在下一節中了解它們的工作原理以及如何實現它們。"
      ]
    }
  ]
}